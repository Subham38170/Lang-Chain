{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb2bcba",
   "metadata": {},
   "source": [
    "# Wikipedia Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0d4ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = WikipediaRetriever(\n",
    "    top_k_results=2,\n",
    "    lang='en'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d55c45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'The geopolitical history of india and pakistan for the perspective of a chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df133108",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03de3001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Indo-Pakistani war of 1971', 'summary': \"The Indo-Pakistani war of 1971, also known as the third Indo-Pakistani war, was a military confrontation between India and Pakistan that occurred during the Bangladesh Liberation War in East Pakistan from 3 December 1971 until the Pakistani capitulation in Dhaka on 16 December 1971.  The war began with Pakistan's Operation Chengiz Khan, consisting of preemptive aerial strikes on eight Indian air stations. The strikes led to India declaring war on Pakistan, marking their entry into the war for East Pakistan's independence, on the side of Bengali nationalist forces. India's entry expanded the existing conflict with Indian and Pakistani forces engaging on both the eastern and western fronts. \\nThirteen days after the war started, India achieved a clear upper hand, and the Eastern Command of the Pakistan military signed the instrument of surrender on 16 December 1971 in Dhaka, marking the formation of East Pakistan as the new nation of Bangladesh. Approximately 93,000 Pakistani servicemen were taken prisoner by the Indian Army, which included 79,676 to 81,000 uniformed personnel of the Pakistan Armed Forces, including some Bengali soldiers who had remained loyal to Pakistan. The remaining 10,324 to 12,500 prisoners were civilians, either family members of the military personnel or collaborators (Razakars).\\nIt is estimated that members of the Pakistani military and supporting pro-Pakistani Islamist militias killed between 300,000 and 3,000,000 civilians in Bangladesh. As a result of the conflict, a further eight to ten million people fled the country to seek refuge in India.\\nDuring the war, members of the Pakistani military and supporting pro-Pakistani Islamist militias called the Razakars raped between 200,000 and 400,000 Bangladeshi women and girls in a systematic campaign of genocidal rape.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Indo-Pakistani_war_of_1971'}, page_content=\"The Indo-Pakistani war of 1971, also known as the third Indo-Pakistani war, was a military confrontation between India and Pakistan that occurred during the Bangladesh Liberation War in East Pakistan from 3 December 1971 until the Pakistani capitulation in Dhaka on 16 December 1971.  The war began with Pakistan's Operation Chengiz Khan, consisting of preemptive aerial strikes on eight Indian air stations. The strikes led to India declaring war on Pakistan, marking their entry into the war for East Pakistan's independence, on the side of Bengali nationalist forces. India's entry expanded the existing conflict with Indian and Pakistani forces engaging on both the eastern and western fronts. \\nThirteen days after the war started, India achieved a clear upper hand, and the Eastern Command of the Pakistan military signed the instrument of surrender on 16 December 1971 in Dhaka, marking the formation of East Pakistan as the new nation of Bangladesh. Approximately 93,000 Pakistani servicemen were taken prisoner by the Indian Army, which included 79,676 to 81,000 uniformed personnel of the Pakistan Armed Forces, including some Bengali soldiers who had remained loyal to Pakistan. The remaining 10,324 to 12,500 prisoners were civilians, either family members of the military personnel or collaborators (Razakars).\\nIt is estimated that members of the Pakistani military and supporting pro-Pakistani Islamist militias killed between 300,000 and 3,000,000 civilians in Bangladesh. As a result of the conflict, a further eight to ten million people fled the country to seek refuge in India.\\nDuring the war, members of the Pakistani military and supporting pro-Pakistani Islamist militias called the Razakars raped between 200,000 and 400,000 Bangladeshi women and girls in a systematic campaign of genocidal rape.\\n\\n\\n== Background ==\\n\\nThe Indo-Pakistani conflict was sparked by the Bangladesh Liberation War, which was a result of the violation of the rights of East Pakistan by the Pakistan Army. The political tensions in East Pakistan had its origin in the creation of Pakistan as a result of the partition of India by the United Kingdom in 1947; the popular language movement in 1950; mass riots in East Bengal in 1964; and the mass protests in 1969. These led to the resignation of President Ayub Khan, who invited army chief General Yahya Khan to take over the central government. The geographical distance between the eastern and western wings of Pakistan was vast; East Pakistan lay over 1,600 kilometres (1,000 mi) away, which greatly hampered any attempt to integrate the Bengali and the Pakistani cultures.\\nTo overcome the Bengali domination and prevent formation of the central government in Islamabad, the controversial One Unit programme established the two wings of East and West Pakistan. West Pakistanis' opposition to these efforts made it difficult to effectively govern both wings. In 1969, President Yahya Khan announced the first general elections and disestablished the status of West Pakistan as a single province in 1970, in order to restore it to its original heterogeneous status comprising four provinces, as defined at the time of establishment of Pakistan in 1947. In addition, there were religious and racial tensions between Bengalis and the multi-ethnic West Pakistanis, as Bengalis looked different from the dominant West Pakistanis.\\nThe East Pakistan's Awami League leader Sheikh Mujibur Rahman stressed his political position by presenting his Six Points and endorsing the Bengalis' right to govern. The 1970 Pakistani general election, resulted in Awami League gaining 167 out of 169 seats for the East Pakistan Legislative Assembly, and a near-absolute majority in the 313-seat National Assembly, while the vote in West Pakistan was mostly won by the socialist Pakistan Peoples Party. The League's election success caused many West Pakistanis to fear that it would allow the Bengalis to draft the constitution based on the six-points and liberalism.\\nTo resol\"),\n",
       " Document(metadata={'title': 'United States aid to Pakistan', 'summary': 'The United States has been providing military aid and economic assistance to Pakistan for various purposes since 1948. In 2017, the U.S. stopped military aid to Pakistan, which was about US$2 billion per year. With U.S. military assistance suspended in 2018 and civilian aid reduced to about $300 million for 2022, Pakistani authorities have turned to other countries for help.', 'source': 'https://en.wikipedia.org/wiki/United_States_aid_to_Pakistan'}, page_content=\"The United States has been providing military aid and economic assistance to Pakistan for various purposes since 1948. In 2017, the U.S. stopped military aid to Pakistan, which was about US$2 billion per year. With U.S. military assistance suspended in 2018 and civilian aid reduced to about $300 million for 2022, Pakistani authorities have turned to other countries for help.\\n\\n\\n== History ==\\nFrom 1947 to 1958, under civilian leadership, the United States provided Pakistan with modest economic aid and limited military assistance. During this period, Pakistan became a member of the South East Asian Treaty Organization (SEATO) and the Central Treaty Organization (CENTO), after a Mutual Defence Assistance Agreement signed in May 1954, which facilitated increased levels of both economic and military aid from the U.S.\\nIn 1958, Ayub Khan led Pakistan's first military coup, becoming Chief Martial Law Administrator (CMLA) and later President until 1969. During his tenure, the U.S. delivered substantial economic and military aid, despite Pakistan's governance by military regime, and amidst events like the Indo-Pakistani war of 1965.\\nYahya Khan succeeded Ayub in 1969, holding power during the Indo-Pakistani war of 1971 which led to the secession of East Pakistan and the formation of Bangladesh. Under Yahya, the U.S. provided adequate economic but minimal military aid.\\nCivilian governance of Zulfikar Ali Bhutto resumed from 1971 to 1977, during which the U.S. offered modest economic support and withheld military aid as Pakistan finalized its constitution, establishing a parliamentary democracy.\\nFollowing another military coup in 1977, Muhammad Zia-ul-Haq led the country. In April 1979, President Jimmy Carter halted all aid, excluding food assistance, due to Pakistan's efforts to establish a uranium enrichment facility, following Symington Amendment. Initial U.S. aid was limited, increasing significantly after geopolitical shifts such as the Soviet invasion of Afghanistan in 1979 and the fall of the Shah of Iran. U.S. sanctions imposed in April 1979 due to Pakistan's nuclear activities were lifted by the end of the year in light of these events.\\nBetween 1988 and 1999, under civilian and democratic governments, U.S. aid was low, particularly after the Soviet withdrawal from Afghanistan in 1989. The aid was suspended in the 1990s under President George H. W. Bush, who cited concerns over Pakistan's developing nuclear program. Relations between the two countries deteriorated as the U.S. implemented the Pressler Amendment. This amendment led to severe sanctions against Pakistan, exacerbating economic challenges for the country's nascent civilian government. Consequently, all forms of bilateral aid from the U.S. to Pakistan were halted. The once expansive operations of the U.S. Agency for International Development (USAID) in Pakistan, which had employed over 1,000 staff across the country, were dramatically reduced almost overnight. Further complications arose from U.S. sanctions following Pakistan's nuclear tests in 1998, which were conducted in response to similar tests by India.\\nThe return of military rule under Pervez Musharraf from 1999 to 2008 initially saw little U.S. economic or military aid. However, following the September 11, 2001 attacks, all U.S. sanctions were removed, and Pakistan, having aligned with the U.S. in the War on Terror, received substantial increases in both economic and military assistance.\\nOn June 16, 2009, the U.S. Senate Foreign Relations Committee passed the Enhanced Partnership with Pakistan Act of 2009, commonly referred to as the Kerry-Lugar Bill. The bipartisan act authorized an annual provision of $1.5 billion in U.S. aid to Pakistan, aimed at fostering enhanced bilateral relations.\\nIn 2011, the Obama administration suspended more than one-third of all military assistance, totaling approximately $800 million due to Osama bin Laden-related controversy. This reduction encompassed funds designated for military h\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9e630e",
   "metadata": {},
   "source": [
    "# Vector Store Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1d86f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "# Step 1: Your source documents\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain helps developers build LLM applications easily.\"),\n",
    "    Document(page_content=\"Chroma is a vector database optimized for LLM-based search.\"),\n",
    "    Document(page_content=\"Embeddings convert text into high-dimensional vectors.\"),\n",
    "    Document(page_content=\"OpenAI provides powerful embedding models.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2188850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c2b356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\AI ML\\aimlvenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "808d88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    collection_name='my_collection'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a259bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vectorstore.as_retriever(search_kwargs = {'k':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67ce24f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is Chroma used for'\n",
    "results = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7da86473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Chroma key', 'summary': 'Chroma key compositing, or chroma keying, is a visual-effects and post-production technique for compositing (layering) two or more images or video streams together based on colour hues (chroma range). The technique has been used in many fields to remove a background from the subject of a photo or video – particularly the newscasting, motion picture, and video game industries. A colour range in the foreground footage is made transparent, allowing separately filmed background footage or a static image to be inserted into the scene. The chroma keying technique is commonly used in video production and post-production. This technique is also referred to as colour keying, colour separation overlay (CSO; primarily by the BBC), or by various terms for specific colour-related variants such as green screen or blue screen; chroma keying can be done with backgrounds of any colour that are uniform and distinct, but green and blue backgrounds are more commonly used because they differ most distinctly in hue from any human skin colour. No part of the subject being filmed or photographed may duplicate the colour used as the backing, or the part may be erroneously identified as part of the backing.\\nIt is commonly used for live weather forecast broadcasts in which a news presenter is seen standing in front of a CGI map instead of a large blue or green background. Chroma keying is also common in the entertainment industry for visual effects in movies and video games. Rotoscopy may instead be carried out on subjects that are not in front of a green (or blue) screen. Motion tracking can also be used in conjunction with chroma keying, such as to move the background as the subject moves.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Chroma_key'}, page_content='Chroma key compositing, or chroma keying, is a visual-effects and post-production technique for compositing (layering) two or more images or video streams together based on colour hues (chroma range). The technique has been used in many fields to remove a background from the subject of a photo or video – particularly the newscasting, motion picture, and video game industries. A colour range in the foreground footage is made transparent, allowing separately filmed background footage or a static image to be inserted into the scene. The chroma keying technique is commonly used in video production and post-production. This technique is also referred to as colour keying, colour separation overlay (CSO; primarily by the BBC), or by various terms for specific colour-related variants such as green screen or blue screen; chroma keying can be done with backgrounds of any colour that are uniform and distinct, but green and blue backgrounds are more commonly used because they differ most distinctly in hue from any human skin colour. No part of the subject being filmed or photographed may duplicate the colour used as the backing, or the part may be erroneously identified as part of the backing.\\nIt is commonly used for live weather forecast broadcasts in which a news presenter is seen standing in front of a CGI map instead of a large blue or green background. Chroma keying is also common in the entertainment industry for visual effects in movies and video games. Rotoscopy may instead be carried out on subjects that are not in front of a green (or blue) screen. Motion tracking can also be used in conjunction with chroma keying, such as to move the background as the subject moves.\\n\\n\\n== History ==\\n\\n\\n=== Predecessors ===\\nPrior to the introduction of travelling mattes and optical printing, double exposure was used to introduce elements into a scene which were not present in the initial exposure. This was done using black draping where a green screen would be used today. George Albert Smith first used this approach in 1898. In 1903, The Great Train Robbery by Edwin S. Porter used double exposure to add background scenes to windows which were black when filmed on set, using a garbage matte to expose only the window areas.\\n\\nIn order to have figures in one exposure actually move in front of a substituted background in the other, a travelling matte was needed, to occlude the correct portion of the background in each frame. In 1918 Frank Williams patented a travelling matte technique, again based on using a black background. This was used in many films, such as The Invisible Man.\\nIn the 1920s, Walt Disney used a white backdrop to include human actors with cartoon characters and backgrounds in his Alice Comedies.\\n\\n\\n=== From bluescreen to greenscreen ===\\nThe blue screen method was developed in the 1930s at RKO Radio Pictures.  At RKO, Linwood Dunn used an early version of the travelling matte to create \"wipes\" – where there were transitions like a windshield wiper in films such as Flying Down to Rio (1933). Credited to Larry Butler, a scene featuring a genie escaping from a bottle was the first use of a proper bluescreen process to create a travelling matte for The Thief of Bagdad (1940), which won the Academy Award for Best Special Effects that year. In 1950, Warner Brothers employee and ex-Kodak researcher Arthur Widmer began working on an ultraviolet travelling matte process. He also began developing bluescreen techniques: one of the first films to use them was the 1958 adaptation of the Ernest Hemingway novella, The Old Man and the Sea, starring Spencer Tracy.\\nThe name \"Chroma-Key\" was RCA\\'s trade name for the process, as used on its NBC television broadcasts, incorporating patents granted to RCA\\'s Albert N. Goldsmith. A very early broadcast use was NBC\\'s George Gobel Show in fall 1957.\\nPetro Vlahos was awarded an Academy Award for his refinement of these techniques in 1964. His technique exploits the fact that most objects in real-world scenes have'),\n",
       " Document(metadata={'title': 'X video extension', 'summary': 'The X video extension, often abbreviated as XVideo or Xv, is a video output mechanism for the X Window System. The protocol was designed by David Carver; the specification for version 2 of the protocol was written in July 1991. It is mainly used today to resize video content in the video controller hardware in order to enlarge a given video or to watch it in full screen mode. Without XVideo, X would have to do this scaling on the main CPU.  That requires a considerable amount of processing power, which could slow down or degrade the video stream; video controllers are specifically designed for this kind of computation, so can do it much more cheaply. Similarly, the X video extension can have the video controller perform color space conversions, and change the contrast, brightness, and hue of a displayed video stream.\\nIn order for this to work, three things have to come together:\\n\\nThe video controller has to provide the required functions.\\nThe device driver software for the video controller and the X display server program have to implement the XVideo interface.\\nThe video playback software has to make use of this interface.\\nMost modern video controllers provide the functions required for XVideo; this feature is known as hardware scaling and YUV acceleration or sometimes as 2D hardware acceleration. The XFree86 X display server has implemented XVideo since version 4.0.2. To check whether a given X display server supports XVideo, one can use the utility xdpyinfo. To check whether the video controller provides the required functions and whether the X device driver implements XVideo for any of them, one can use the xvinfo program.\\nVideo playback programs that run under the X Window system, such as MPlayer, MythTV or xine, typically have an option to enable XVideo output. It is very advisable to switch on this option if the system GPU video-hardware and device drivers supports XVideo and more modern rendering systems such as OpenGL and VDPAU are unavailable – the speedup is very noticeable even on a fast CPU.\\nWhile the protocol itself has features for reading and writing of video streams from and to video adapters, in practice today only the functions XvPutImage and XvShmPutImage are used: the client program repeatedly prepares images and passes them on to the graphics hardware to be scaled, converted and displayed.', 'source': 'https://en.wikipedia.org/wiki/X_video_extension'}, page_content='The X video extension, often abbreviated as XVideo or Xv, is a video output mechanism for the X Window System. The protocol was designed by David Carver; the specification for version 2 of the protocol was written in July 1991. It is mainly used today to resize video content in the video controller hardware in order to enlarge a given video or to watch it in full screen mode. Without XVideo, X would have to do this scaling on the main CPU.  That requires a considerable amount of processing power, which could slow down or degrade the video stream; video controllers are specifically designed for this kind of computation, so can do it much more cheaply. Similarly, the X video extension can have the video controller perform color space conversions, and change the contrast, brightness, and hue of a displayed video stream.\\nIn order for this to work, three things have to come together:\\n\\nThe video controller has to provide the required functions.\\nThe device driver software for the video controller and the X display server program have to implement the XVideo interface.\\nThe video playback software has to make use of this interface.\\nMost modern video controllers provide the functions required for XVideo; this feature is known as hardware scaling and YUV acceleration or sometimes as 2D hardware acceleration. The XFree86 X display server has implemented XVideo since version 4.0.2. To check whether a given X display server supports XVideo, one can use the utility xdpyinfo. To check whether the video controller provides the required functions and whether the X device driver implements XVideo for any of them, one can use the xvinfo program.\\nVideo playback programs that run under the X Window system, such as MPlayer, MythTV or xine, typically have an option to enable XVideo output. It is very advisable to switch on this option if the system GPU video-hardware and device drivers supports XVideo and more modern rendering systems such as OpenGL and VDPAU are unavailable – the speedup is very noticeable even on a fast CPU.\\nWhile the protocol itself has features for reading and writing of video streams from and to video adapters, in practice today only the functions XvPutImage and XvShmPutImage are used: the client program repeatedly prepares images and passes them on to the graphics hardware to be scaled, converted and displayed.\\n\\n\\n== Display ==\\nAfter video has been scaled and prepared for display on the video card, it must be displayed. There are a few possible ways to display accelerated video at this stage. Since full acceleration means that the video controller is responsible for scaling, converting, and drawing the video, the technique used depends entirely on what the video is being drawn onto.\\n\\n\\n=== The role of window manager support and compositing ===\\nUnder X, how video is finally drawn depends largely on the X window manager in use. With properly installed drivers, and GPU hardware such as supported Intel, ATI, and nVidia chip sets, some window managers, called compositing window managers, allow windows to be separately processed and then rendered (or composited). This involves all windows being rendered to separate output buffers in memory first, and later combined to form a complete graphical interface. While in (video) memory, individual windows can be transformed separately, and accelerated video may be added at this stage using a texture filter, before the window is composited and drawn. XVideo can also be used to accelerate video playback during the drawing of windows using an OpenGL Framebuffer Object or pbuffer.\\nMetacity, an X window manager uses compositing in this way. The compositing can also make use of 3D pipelines accelerations such as GLX_EXT_texture_from_pixmap. Among other things, this process allows many video outputs to share the same screen without interfering with each other. Other compositing window managers such as Compiz also use compositing.\\nHowever, on a system with limited OpenGL acceleration function, specifi')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "469bb21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 ---\n",
      "Chroma key compositing, or chroma keying, is a visual-effects and post-production technique for compositing (layering) two or more images or video streams together based on colour hues (chroma range). The technique has been used in many fields to remove a background from the subject of a photo or video – particularly the newscasting, motion picture, and video game industries. A colour range in the foreground footage is made transparent, allowing separately filmed background footage or a static image to be inserted into the scene. The chroma keying technique is commonly used in video production and post-production. This technique is also referred to as colour keying, colour separation overlay (CSO; primarily by the BBC), or by various terms for specific colour-related variants such as green screen or blue screen; chroma keying can be done with backgrounds of any colour that are uniform and distinct, but green and blue backgrounds are more commonly used because they differ most distinctly in hue from any human skin colour. No part of the subject being filmed or photographed may duplicate the colour used as the backing, or the part may be erroneously identified as part of the backing.\n",
      "It is commonly used for live weather forecast broadcasts in which a news presenter is seen standing in front of a CGI map instead of a large blue or green background. Chroma keying is also common in the entertainment industry for visual effects in movies and video games. Rotoscopy may instead be carried out on subjects that are not in front of a green (or blue) screen. Motion tracking can also be used in conjunction with chroma keying, such as to move the background as the subject moves.\n",
      "\n",
      "\n",
      "== History ==\n",
      "\n",
      "\n",
      "=== Predecessors ===\n",
      "Prior to the introduction of travelling mattes and optical printing, double exposure was used to introduce elements into a scene which were not present in the initial exposure. This was done using black draping where a green screen would be used today. George Albert Smith first used this approach in 1898. In 1903, The Great Train Robbery by Edwin S. Porter used double exposure to add background scenes to windows which were black when filmed on set, using a garbage matte to expose only the window areas.\n",
      "\n",
      "In order to have figures in one exposure actually move in front of a substituted background in the other, a travelling matte was needed, to occlude the correct portion of the background in each frame. In 1918 Frank Williams patented a travelling matte technique, again based on using a black background. This was used in many films, such as The Invisible Man.\n",
      "In the 1920s, Walt Disney used a white backdrop to include human actors with cartoon characters and backgrounds in his Alice Comedies.\n",
      "\n",
      "\n",
      "=== From bluescreen to greenscreen ===\n",
      "The blue screen method was developed in the 1930s at RKO Radio Pictures.  At RKO, Linwood Dunn used an early version of the travelling matte to create \"wipes\" – where there were transitions like a windshield wiper in films such as Flying Down to Rio (1933). Credited to Larry Butler, a scene featuring a genie escaping from a bottle was the first use of a proper bluescreen process to create a travelling matte for The Thief of Bagdad (1940), which won the Academy Award for Best Special Effects that year. In 1950, Warner Brothers employee and ex-Kodak researcher Arthur Widmer began working on an ultraviolet travelling matte process. He also began developing bluescreen techniques: one of the first films to use them was the 1958 adaptation of the Ernest Hemingway novella, The Old Man and the Sea, starring Spencer Tracy.\n",
      "The name \"Chroma-Key\" was RCA's trade name for the process, as used on its NBC television broadcasts, incorporating patents granted to RCA's Albert N. Goldsmith. A very early broadcast use was NBC's George Gobel Show in fall 1957.\n",
      "Petro Vlahos was awarded an Academy Award for his refinement of these techniques in 1964. His technique exploits the fact that most objects in real-world scenes have\n",
      "\n",
      "--- Result 2 ---\n",
      "The X video extension, often abbreviated as XVideo or Xv, is a video output mechanism for the X Window System. The protocol was designed by David Carver; the specification for version 2 of the protocol was written in July 1991. It is mainly used today to resize video content in the video controller hardware in order to enlarge a given video or to watch it in full screen mode. Without XVideo, X would have to do this scaling on the main CPU.  That requires a considerable amount of processing power, which could slow down or degrade the video stream; video controllers are specifically designed for this kind of computation, so can do it much more cheaply. Similarly, the X video extension can have the video controller perform color space conversions, and change the contrast, brightness, and hue of a displayed video stream.\n",
      "In order for this to work, three things have to come together:\n",
      "\n",
      "The video controller has to provide the required functions.\n",
      "The device driver software for the video controller and the X display server program have to implement the XVideo interface.\n",
      "The video playback software has to make use of this interface.\n",
      "Most modern video controllers provide the functions required for XVideo; this feature is known as hardware scaling and YUV acceleration or sometimes as 2D hardware acceleration. The XFree86 X display server has implemented XVideo since version 4.0.2. To check whether a given X display server supports XVideo, one can use the utility xdpyinfo. To check whether the video controller provides the required functions and whether the X device driver implements XVideo for any of them, one can use the xvinfo program.\n",
      "Video playback programs that run under the X Window system, such as MPlayer, MythTV or xine, typically have an option to enable XVideo output. It is very advisable to switch on this option if the system GPU video-hardware and device drivers supports XVideo and more modern rendering systems such as OpenGL and VDPAU are unavailable – the speedup is very noticeable even on a fast CPU.\n",
      "While the protocol itself has features for reading and writing of video streams from and to video adapters, in practice today only the functions XvPutImage and XvShmPutImage are used: the client program repeatedly prepares images and passes them on to the graphics hardware to be scaled, converted and displayed.\n",
      "\n",
      "\n",
      "== Display ==\n",
      "After video has been scaled and prepared for display on the video card, it must be displayed. There are a few possible ways to display accelerated video at this stage. Since full acceleration means that the video controller is responsible for scaling, converting, and drawing the video, the technique used depends entirely on what the video is being drawn onto.\n",
      "\n",
      "\n",
      "=== The role of window manager support and compositing ===\n",
      "Under X, how video is finally drawn depends largely on the X window manager in use. With properly installed drivers, and GPU hardware such as supported Intel, ATI, and nVidia chip sets, some window managers, called compositing window managers, allow windows to be separately processed and then rendered (or composited). This involves all windows being rendered to separate output buffers in memory first, and later combined to form a complete graphical interface. While in (video) memory, individual windows can be transformed separately, and accelerated video may be added at this stage using a texture filter, before the window is composited and drawn. XVideo can also be used to accelerate video playback during the drawing of windows using an OpenGL Framebuffer Object or pbuffer.\n",
      "Metacity, an X window manager uses compositing in this way. The compositing can also make use of 3D pipelines accelerations such as GLX_EXT_texture_from_pixmap. Among other things, this process allows many video outputs to share the same screen without interfering with each other. Other compositing window managers such as Compiz also use compositing.\n",
      "However, on a system with limited OpenGL acceleration function, specifi\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fa647a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search(query, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e8b09b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Chroma is a vector database optimized for LLM-based search.'),\n",
       " Document(metadata={}, page_content='LangChain helps developers build LLM applications easily.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8618d7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 ---\n",
      "Chroma is a vector database optimized for LLM-based search.\n",
      "\n",
      "--- Result 2 ---\n",
      "LangChain helps developers build LLM applications easily.\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e81da1",
   "metadata": {},
   "source": [
    "# MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b168c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain makes it easy to work with LLMs.\"),\n",
    "    Document(page_content=\"LangChain is used to build LLM based applications.\"),\n",
    "    Document(page_content=\"Chroma is used to store and search document embeddings.\"),\n",
    "    Document(page_content=\"Embeddings are vector representations of text.\"),\n",
    "    Document(page_content=\"MMR helps you get diverse results when doing similarity search.\"),\n",
    "    Document(page_content=\"LangChain supports Chroma, FAISS, Pinecone, and more.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a312eec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a7c790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec5ac336",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type='mmr',\n",
    "    search_kwargs={'k':3,'lambda_mult':0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24289320",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is langchain ?' \n",
    "results = retriver.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a271148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='LangChain helps developers build LLM applications easily.'),\n",
       " Document(metadata={}, page_content='Chroma is a vector database optimized for LLM-based search.')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f77f79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 ---\n",
      "LangChain helps developers build LLM applications easily.\n",
      "\n",
      "--- Result 2 ---\n",
      "Chroma is a vector database optimized for LLM-based search.\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f404f446",
   "metadata": {},
   "source": [
    "# Multiquery Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b70512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssahu\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import BaseDocumentTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d1628",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community.retrievers.multi_query'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WikipediaRetriever\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# or\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmulti_query\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiQueryRetriever\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_community.retrievers.multi_query'"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "# or\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af205200",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community.retrievers.multi_query'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmulti_query\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiQueryRetriever\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_community.retrievers.multi_query'"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers.multi_query import MultiQueryRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f04cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m help(\u001b[43mretriever\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48781789",
   "metadata": {},
   "source": [
    "# Contextual Compression Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ff12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
